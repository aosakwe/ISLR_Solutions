---
title: "Chapter 3 Exercises"
output:
  pdf_document: default
  html_notebook: default
author: "Adrien Osakwe"
---


# Conceptual



### 1)

  Table 3.4 shows basic stats from  the linear model for the
  estimation of the number of units sold given the advertising budgets for (the 
  predictors): *TV, radio and newspaper*. The null hypotheses the p-values 
  correspond to are that there is no link between the budgets of each form of
  advertisement and the number of units sold ( == the coefficients of the 
  predictors are equal to 0).
  
  Based on this relationship, the p-values indicate that the null hypotheses for
  both the TV and radio budgets' relationship to sales can be rejected. However,
  the null hypothesis for newspaper must be accepted as its p-value is much
  larger than 0.05. We would therefore conclude that TV and radio budgets are 
  two predictors worth using to estimate sales.
  
### 2) 

  The KNN classifier will be used to estimate a qualitative response,  (a 
  category). This functions as a conditional probability where we find the k
  nearest neighbors to our observation and determine what proportion of the 
  neighbors are in a given class. i.e: If k-1/k of our observation's neig-
  hbors are in class Green, then we would classify our observation as green with
  a conditional probability of k-1/k.
  
  The KNN regression method is used to estimate quantitative responses, where
  the estimation of f(x) will be the average response of the k nearest-neighbors
  . i.e: if the average of our observation's neighborhood response is 8, then
  our regression model with k neighbors will estimate the response as 8.

### 3)

  The linear model can be written as follows:
  
  y = 50 + 20GPA + 0.07IQ + 35Level + 0.01(GPA x IQ) -10(GPA x Level)
  
  We can simplify the model as the following:
  Level = 1: College graduate
  y = 85 + 10GPA + 0.07IQ + 0.01(GPA x IQ)
  Level = 0: High School Graduate
  y = 50 + 20GPA + 0.07IQ +0.01(GPA X IQ)
  
  --> These models differ at two coefficients: the intercept is 35 units larger
  for a college graduate and the GPA coefficient is twice as large for the 
  high school graduate

A) 
    Based on the different coefficients in the two instances in the model, the
    student with the higher salary is dependent on GPA. Because the intercept is
    larger for the college instance, the college graduate will have a higher
    salary for low GPAs. However, when GPA is 3.5, both students have the same 
    salary (10GPA =35, which equals the difference in the intercept). Therefore,
    GPA > 3.5 will lead to a higher salary for a high school graduate.
    
    The correct answer is therefore (iii)
    
B) 
  The salary will be 137100 dollars. What did they study? How is it so high?!! 
  Who knows...


C)
  This is false. Evidence for an interaction effect would be based on the 
  t-statistic and resulting p-value for the interaction's coefficient to know if
  we can accept the null hypothesis that there is no interaction effect.

### 4)

  a. There is not enough information to know as the cubic regression may enable
  the model to overfit more to any noise in the data. On the other hand, since
  the linear model is closer to the truth, it may also have a lower RSS.
  b. We expect it to be lower for the linear regression as its assumption are 
  closest to the true trend, reducing reducible error. The cubic model is
  more likely to have overfit, increasing reducible error.
  c. There is not enough information as the true trend may not be cubic either,
  in which case RSS will also be larger for the cubic regression. If anything,
  the size of the RSS for either may help give a clue of what kind of 
  non-linear relationship is the truth.
  d.  As in c, little can be said here without knowing what the true 
  relationship is. Therefore, it is most likely higher in the regression 
  furthest from the truth.
  

### 5)
Note: I really need to review LaTex...

  y(i) = x(i)sum(x(j)y(j))/sum(x^2(i'))
  
  y(i) = sum(x(i)x(j)y(j))/sum(x^2(i'))
  
  y(i) = sum( ((x(i)x(j))/sum(x^2(i')))y(j))
  
  y(i) = sum(a(j)y(j))
  
  a(j) = x(i)(j)/sum(x^2(i'))

### 6)
  
  y = b0 + b1x
  
  x = x(^)
  
  y = b0 + b1x(^)
  
  b0 = y(^) - b1x(^)
  
  y = y(^)

### 7)

``` {r Q7, echo = TRUE}
setwd(getwd())
```

![Part 1 ](Q7_1.jpeg)
![Part 2 ](Q7_2.jpeg)



# Applied

### Packages

``` {r packages, echo = TRUE}
library(ISLR2)
```

### 8)

A)

 i) The summary of lm.fit shows that there is a relationship between mpg and
     horsepower (very small p-value).
 
 ii) According to the R^2 value, horsepower accounts for 60% of the variance in 
     mpg.
     Note: I saw in the solutions manual afterwards that we can also check the
     percentage error (RSE/mean response) to evaluate relationship strength.

iii) Negative relationship
 
 iv) Prediction: mpg ~ 24.47, CI = [23.97-24.96], PI = [14.81-34.12] 
 (rounded to 2 d.p.)

``` {r Q8, echo = TRUE}
# Part A
lm.fit <- lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
predict(lm.fit, data.frame(horsepower = 98), interval = "confidence")
predict(lm.fit, data.frame(horsepower = 98), interval = "prediction")
```

B) 
  
  See plot below

``` {r Q8B, echo = TRUE}
# Part B
attach(Auto)
plot(horsepower,mpg) + title("Part B - Mpg vs.Horsepower")
abline(lm.fit, col = "blue")
```

C)
  
  Based on the residuals v. fitted value plot, we can see that the true 
  relationship is not linear, indicating that some for of transformation will be
  required to best fit the data. There is not a large fat tail in the Q-Q plot
  which indicates that the responses are not very non-normal. The remaining
  plots highlight one or two observations that are outliers.
  
``` {r Q8C, echo = TRUE}
# Part C
par(mfrow = c(2,2))
plot(lm.fit) 
```
 
### 9)

A) 

  See plot below.
  
``` {r Q9A, echo = TRUE}
#Part A
pairs(Auto)
```

B)
  
  See matrix below.
  
``` {r Q9B, echo = TRUE}
#Part B
mat.cor <- cor(Auto[,!(names(Auto) == "name")])
data.frame(mat.cor)
```

C)
  
  i) The F-statistic is below 0.05 which confirms we can reject the null
  hypothesis. 
  
  ii) Based on the p-values, only some of the predictors are 
  shown to have a relationship with the response. These are: displacement, 
  weight, year, and origin.
  
  iii)The coefficient for year suggests that as the year increases, the mpg
  increases. This is reassuring to see as I would be concerned if more 
  technologically advanced cars were incapable of a better mileage than a model
  T... 


``` {r Q9C, echo = TRUE}
#Part C
lm.fit <- lm(mpg ~ . - name, data = Auto)
summary(lm.fit)
```

D)

  The residuals vs leverage plot shows observation 14 to be a considerable
  outlier compared to the rest of the dataset. Observations 323,326 and 327 
  are also shown to be outliers based on the other three plots.
  The residuals vs. fitted plot seems to have a very light non-linearity. Q-Q 
  plot shows as fat tail indicating some of the data is non-normal.

``` {r Q9D, echo = TRUE}
#Part D
par(mfrow = c(2,2))
plot(lm.fit)
```

E)
  
  Checked for interactions effects between cylinders and weight as well as 
  horsepower and acceleration. Both interactions are shown to have a significant
  relationship. In addition this model reduces RSE and increases R^2. An anova
  analysis reveals that this model is a significant improvement of the previous 
  one.

``` {r Q9E, echo = TRUE}
#Part E
lm.fit2 <- lm(mpg ~ . - name + cylinders:weight + horsepower:acceleration, data = Auto)
summary(lm.fit2)
anova(lm.fit,lm.fit2)
```

F)
  Log-transforming mpg seems ot better fit the data.RSE is very small and the
  residuals v. fitted plot shows no trend. A square root transform also reduces
  R^2, however the non-linear trend in the residuals vs. fitted plot shows the
  model does not fit the data as well as the log transformation.
  See plots below
  
``` {r Q9F-log, echo = TRUE}
#Part F
log.fit <- lm(log(mpg) ~ . - name, data = Auto)
summary(log.fit)
par(mfrow = c(2,2))
plot(log.fit)
```

``` {r Q9F-sqrt, echo = TRUE}
sqrt.fit <-lm(sqrt(mpg) ~ . - name, data = Auto)
summary(sqrt.fit)
par(mfrow = c(2,2))
plot(sqrt.fit)
```

### 10)
A)
``` {r Q10A, echo = TRUE}
lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(lm.fit)
```

B)
  
  See above for the summary statistics of the model.
  
  Based on the summary, the F-statistic indicates that we can reject then null
  hypothesis that all coefficients are 0. The individual p-values show that
  Urban is the only predictor to not be related to sales. 
  The coefficient of Price tells us that a one unit increase in price will 
  reduce sales by 0.05 units. The coefficient of US tells us that being in the
  US increases sales by 1.2 units. 
  
C)
  
  If in the US: Sales = 14.2 -0.05Price 
  Else: Sales = 13 - 0.05Price

D)
  
  As mentioned in B, the null hypothesis can be rejected for Price and US.

E)
  
``` {r Q10E, echo = TRUE}
lm.fit2 <- lm(Sales ~ Price + US, data = Carseats)
summary(lm.fit2)
```
F)
  
  The summary stats for both models tell us that the predictors in both cases
  only account for about 23% of the variance in sales. Based off this alone,
  we will likely have to use other predictors and/or interaction effects to have
  a better fit.

G)

``` {r Q10G, echo = TRUE}
confint(lm.fit2)
```
  
H)
  
  The diagnostics show some evidence for one outlier (377) and one case of high 
  leverage.
``` {r Q10H, echo = TRUE}
par(mfrow = c(2,2))
plot(lm.fit2)
```


### 11) 

A)

  Coefficients: x
  
  Estimate      SE    t value Pr(>|t|)  
  
  1.9939     0.1065   18.73   <2e-16 
  
  These results show that the null hypothesis can be rejected as the p-value is
  below 0.05.

``` {r Q11A, echo = TRUE}
set.seed(1)
x <- rnorm(100)
y <- 2*x +rnorm(100)
lm.fit <- lm(y ~ x + 0)
summary(lm.fit)
```

B)

  Coefficients: y
  
  Estimate Std. Error t value Pr(>|t|)    
  
  0.39111    0.02089   18.73   <2e-16 

  Much like before, we can reject the null hypothesis as p-value is below 0.05.

``` {r Q11B, echo = TRUE}
set.seed(1)
x <- rnorm(100)
y <- 2*x +rnorm(100)
lm.fit <- lm(x ~ y + 0)
summary(lm.fit)
```

C)

  Both results have the same t-statistic and p-value. This is expected as both 
  models are producing the same line (we simply inverted the roles of predictor
  and response).


D)

![Q11D Solution](Q9_1.jpeg)0

``` {r Q11D, echo = TRUE}
true_t <- 18.73
test_t <- (sqrt(length(x)-1)*sum(x %*% y))/(sqrt(sum(y**2)*sum(x**2)-sum(x %*% y)**2))
true_t == round(test_t,2)
```
E)

  Based on the derived formula, and that there are n observation of x and n 
  observations of y, we know that the t-statistic is the same for regressions
  of x onto y and y onto x. This is because the formula shows that the values of
  x and y undergo the exact same transformations in the formula whether they are
  the response or the predictor. Therefore their exact role in the regression is
  irrelevant as it will lead to the same value.
  
F)
  
``` {r Q11F, echo = TRUE}
set.seed(1)
x <- rnorm(100)
y <- 2*x +rnorm(100)
lm.fit <- lm(x ~ y)
lm.fit2 <- lm(y ~ x)
round(summary(lm.fit)$coefficients[,3][2],2) == round(summary(lm.fit2)$coefficients[,3][2],2)

```

### 12)

A)

  The equation for the coefficients are:
  
    y onto x:
    
      Sum(x %*% y)/sum(x**2)
    
    x onto y:
    
      Sum(y %*% x)/sum(y**2)
      
  Based on the above, the conditions are the same if x and y are equal.

B)
  
``` {r Q12B, echo = TRUE}
set.seed(1)
x <- rnorm(100)
y <- 2*x +rnorm(100)
lm.fit <- lm(x ~ y)
lm.fit2 <- lm(y ~ x)
round(lm.fit$coefficients[2],2) == round(lm.fit2$coefficients[2],2)
```
C)

``` {r Q12C, echo = TRUE}
set.seed(1)
x <- rnorm(100)
y <- x
lm.fit <- lm(x ~ y)
lm.fit2 <- lm(y ~ x)
round(lm.fit$coefficients[2],2) == round(lm.fit2$coefficients[2],2)
```




### 13)

``` {r Q13A-C, echo = TRUE}
set.seed(1)

x <- rnorm(100)
eps <- rnorm(100,0,0.25)
y <- -1 + 0.5*x + eps

```

C)
  
  Y has a length of 100 observations, with B0 as -1 and B1 as 0.5.

D)

  Scatterplot (see below) shows a postive linear correlation between x and y.
  
``` {r Q13D, echo = TRUE}
plot(x,y)
```


E)
  
  The least squares linear model (below) finds coefficients that closely 
  ressemble their true values.
``` {r Q13E, echo = TRUE}
lm.fit <- lm(y ~ x)
summary(lm.fit)
```

F)

  We can see below that the model seems to fit the data very well as it matches
  the population regression line.

``` {r Q13F, echo = TRUE}
plot(x,y)
abline(lm.fit, col = "red")
abline(-1,0.5, col = "blue")
legend("topleft",c("Least Squares", "Population Regression"), col = c("red", "blue"), lty = c(1,1))
```
G)
  Running anova on the two models indicates that there is no significant 
  improvement when including X**2 in the model.
``` {r Q12G, echo = TRUE}
lm.fit2 <- lm(y ~ x + I(x**2))
anova(lm.fit,lm.fit2)
```

H)

  Reduced the noise of data by generating the epsilon vector using a normal
  distribution with a smaller standard deviation.
  
  This, as expected, generates a model that matches the trend of the test data
  much more than the model used on the noisier data.
``` {r Q13H, echo = TRUE}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100,0,0.05)
y <- -1 + 0.5*x + eps
lm.fit2 <- lm(y ~ x)
plot(x,y)
abline(lm.fit2, col = "red")
abline(-1,0.5, col = "blue")
legend("topleft",c("Least Squares", "Population Regression"), col = c("red", "blue"), lty = c(1,1))
```

I)

  Increasing standard deviation in the normal distribution used for eps 
  increases noise.
  
  As expected, there is much more deviation between the observed responses and
  what our model predicts.
``` {r Q13I, echo = TRUE}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100,0,0.5)
y <- -1 + 0.5*x + eps
lm.fit3 <- lm(y ~ x)
plot(x,y)
abline(lm.fit3, col = "red")
abline(-1,0.5, col = "blue")
legend("topleft",c("Least Squares", "Population Regression"), col = c("red", "blue"), lty = c(1,1))
```
J)

  As seen below, the range of our CI increases with noise for beta1.
  
``` {r Q13J, echo = TRUE}
confint(lm.fit)
confint(lm.fit2)
confint(lm.fit3)
```


### 14) 

A)
  
  The model has the form:
    
    y = 2+ 2x1 + 0.3x2 + e
  
  b0 = 2, b1 = 2, b2 = 0.3

``` {r Q14A, echo = TRUE}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 +rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 +rnorm(100)
```

B)

  x1 and x2 share a positive correlation (every unit increase in x1 increases
  x2 by 0.5 units)
  
``` {r Q14B, echo = TRUE}
plot(x1,x2)
```

C)

  F-statistic indicates that we can reject the null hypothesis and conclude that
  there is a link between the variables and y. The p values from the t-stats 
  tell us that we can reject the null hypothesis for x1 but must accept it for
  x2 as its p-value is greater than 0.05.
``` {r Q14C, echo = TRUE}
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
```

D)
  
  Removing x2 barely changes R^2 and the RSE and causes the F-statistic to 
  increase. We can still reject the null hypothesis that b1 = 0.
``` {r Q14D, echo = TRUE}
lm.fit2 <- lm(y ~ x1)
summary(lm.fit2)
```


E)

  When we use x2 instead of x1, we can also reject the null hypothesis for b1,
  giving more evidence that these two variables are collinear.
``` {r Q14E, echo = TRUE}
lm.fit3 <- lm(y ~ x2)
summary(lm.fit3)
```

F)

  At first glance, one would say that the results are contradictory. However,
  given what we know about collinearity and that we know x2 depends on x1, these
  results make sense as x2 provides little information to the model when x1 is
  already present but is meaningful if it is not. 

G)

  This mismeasured data point now masks the importance of x1 and unmasks that of
  x2 in the MLR.
  
  Diagnostics of the three new models show the observation to be an outlier and
  /or a high leverage point.

``` {r Q14G, echo = TRUE}
x1 <- c(x1,0.1)
x2 <- c(x2,0.8)
y <- c(y,6)


lm.fit4 <- lm(y ~ x1 + x2)
summary(lm.fit4)
lm.fit5 <- lm(y ~ x2)
lm.fit6 <- lm(y ~ x1)
par(mfrow = c(2,2))
plot(lm.fit4)

par(mfrow = c(2,2))
plot(lm.fit5)

par(mfrow = c(2,2))
plot(lm.fit6)
```





### 15)


A)
  
  Based on the summary stats, all of the variables can have their null 
  hypothesis rejected except for the predictor "chas" for which the p-values 
  for the corresponding t and f statistics are above 0.05.
  
  The predictor "rad" explained the most variance.

``` {r Q15A, echo = TRUE}
models <- lapply(Boston[,-1],function(a) lm(crim ~ a,data = Boston))

lapply(models,function(a) summary(a))

```

B)

  Based on the summary stats, we can reject the null hypothesis for the 
  coefficients of: zn,dis,rad and medv.
``` {r Q15B, echo = TRUE}
mlr.fit <- lm(crim ~., data = Boston)
summary(mlr.fit)
```
C)
  For most predictors, our plot is showing a similar coefficient. However, one
  predictor, nox, has drastically different coefficients. This is an
  indicator of a possible interaction effect with nox and another variable. As 
  the slr only consider the effect of the one predictor, and the mlr considers
  the effect of one predictor at fixed values of the other predictors, the model
  would not have been able to account for this interaction as is. We could look
  at a matrix of correlations to see if there is additional evidence there (need
  to remove chas which is a qualitative predictor).Sure enough there are some
  high correlations for some of these predictors.

``` {r Q15C, echo = TRUE}
x <- sapply(models, function(a) summary(a)$coefficients[2,1])
y <- summary(mlr.fit)$coefficients[-1,1]
plot(x,y)
text(x,y, labels = names(models))

cor(Boston[-c(1,4)])
```


D)

  Briefly skimming over the results shows certain predictors to have polynomial
  coefficients whose null hypothesis were rejected. Some of these are nox, indus
  , ptratio and medv. 
``` {r Q15D, echo = TRUE}
models2 <- lapply(Boston[,-1],function(a) lm(crim ~ a + I(a**2) + I(a**3),data = Boston))
lapply(models2,function(a) summary(a))
```